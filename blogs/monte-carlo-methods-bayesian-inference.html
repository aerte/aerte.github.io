<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Monte Carlo Methods in Bayesian Inference - Felix Aertebjerg</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'space-grotesk': ['Space Grotesk', 'sans-serif'],
                    },
                    colors: {
                        'accent-purple': '#8b5cf6',
                        'accent-blue': '#3b82f6',
                        'accent-cyan': '#06b6d4',
                        'accent-pink': '#ec4899',
                        'accent-orange': '#f59e0b',
                    }
                }
            }
        }
    </script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <!-- MathJax for math formulas -->
    <script>
        window.MathJax = {
            tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]},
            svg: {fontCache: 'global'}
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <style>
        /* Custom scrollbar */
        ::-webkit-scrollbar {
            width: 10px;
        }
        ::-webkit-scrollbar-track {
            background: #ffffff;
        }
        ::-webkit-scrollbar-thumb {
            background: #e2e8f0;
            border-radius: 5px;
        }
        ::-webkit-scrollbar-thumb:hover {
            background: #8b5cf6;
        }

        /* Selection */
        ::selection {
            background: rgba(139, 92, 246, 0.2);
            color: #1a202c;
        }

        /* Prose styling for blog content */
        .prose {
            color: #374151;
        }
        .prose h4 {
            font-family: 'Space Grotesk', sans-serif;
            font-weight: 600;
            color: #1f2937;
            margin-bottom: 0.75rem;
        }
        .prose p {
            margin-bottom: 1rem;
        }
        .prose pre {
            background: #1f2937 !important;
            border-radius: 0.5rem;
            padding: 1rem;
            overflow-x: auto;
            margin: 1.5rem 0;
        }
        .prose code {
            background: #f1f5f9;
            padding: 0.125rem 0.25rem;
            border-radius: 0.25rem;
            font-size: 0.875em;
            font-family: 'Courier New', monospace;
        }
        .prose pre code {
            background: transparent;
            padding: 0;
            border-radius: 0;
        }

        /* Smooth scrolling */
        html {
            scroll-behavior: smooth;
        }
    </style>
</head>
<body class="bg-white text-slate-900 font-space-grotesk font-medium leading-relaxed min-h-screen antialiased">
    <!-- Navigation -->
    <nav class="bg-white border-b border-slate-200 sticky top-0 z-10">
        <div class="max-w-4xl mx-auto px-4 py-3">
            <div class="flex justify-between items-center">
                <a href="../index.html" class="text-accent-purple font-bold text-lg hover:text-purple-700 transition-colors">Felix Aertebjerg</a>
                <div class="flex space-x-6">
                    <a href="../index.html" class="text-slate-600 hover:text-accent-purple transition-colors">Home</a>
                    <a href="index.html" class="text-slate-600 hover:text-accent-purple transition-colors">Blog</a>
                    <a href="../index.html#about" class="text-slate-600 hover:text-accent-purple transition-colors">About</a>
                </div>
            </div>
        </div>
    </nav>

    <div class="max-w-4xl mx-auto px-4 py-8">
        <!-- Article Header -->
        <header class="mb-8">
            <div class="flex items-center gap-4 text-sm text-slate-600 mb-4">
                <time datetime="2025-01-05">January 5, 2025</time>
                <span class="flex items-center gap-1">
                    <i class="fa-solid fa-tags"></i>
                    <span>Bayesian Statistics, MCMC, Probabilistic Programming</span>
                </span>
                <span class="text-slate-400">‚Ä¢</span>
                <span>12 min read</span>
            </div>
            <h1 class="font-space-grotesk text-4xl font-bold text-slate-900 mb-6 leading-tight">
                Monte Carlo Methods in Bayesian Inference
            </h1>
            <p class="text-xl text-slate-600 leading-relaxed">
                Monte Carlo methods have revolutionized Bayesian inference by enabling us to approximate complex posterior distributions
                that would otherwise be analytically intractable. This post explores the fundamental concepts behind these powerful techniques.
            </p>
        </header>

        <!-- Article Content -->
        <article class="prose prose-lg max-w-none">
            <h4 class="font-space-grotesk text-2xl font-semibold text-slate-800 mb-4">The Monte Carlo Principle</h4>
            <p>
                At its core, Monte Carlo integration approximates expectations by averaging function evaluations at randomly sampled points.
                For Bayesian inference, this allows us to compute posterior expectations without knowing the normalizing constant.
            </p>

            <div class="bg-slate-50 p-6 rounded-lg my-8 border-l-4 border-accent-blue">
                <p class="text-sm text-slate-600 mb-2 font-medium">The basic Monte Carlo estimator:</p>
                <div class="text-center text-lg font-mono bg-white p-4 rounded border">\(
                    \mathbb{E}[f(\theta) | \mathbf{y}] \approx \frac{1}{S} \sum_{s=1}^S f(\theta^{(s)})
                \)</div>
                <p class="text-sm text-slate-600 mt-2">where \(\theta^{(s)} \sim p(\theta | \mathbf{y})\)</p>
            </div>

            <h4 class="font-space-grotesk text-2xl font-semibold text-slate-800 mb-4">Markov Chain Monte Carlo (MCMC)</h4>
            <p>
                While basic Monte Carlo requires independent samples from the target distribution, MCMC methods generate correlated samples
                that converge to the target distribution as the chain runs longer.
            </p>

            <h5 class="font-space-grotesk text-lg font-semibold text-slate-800 mb-3">Metropolis-Hastings Algorithm</h5>
            <p>The Metropolis-Hastings algorithm is one of the most fundamental MCMC methods:</p>

            <pre class="language-python"><code>import numpy as np
from scipy.stats import norm

def metropolis_hastings(target_logpdf, proposal_sampler, x0, n_samples, burnin=1000):
    """
    Metropolis-Hastings MCMC sampler

    Parameters:
    - target_logpdf: log probability density function of target distribution
    - proposal_sampler: function that samples from proposal distribution
    - x0: initial state
    - n_samples: number of samples to generate
    - burnin: number of initial samples to discard
    """
    samples = []
    x = x0
    accepted = 0

    for i in range(n_samples + burnin):
        # Propose new state
        x_prop = proposal_sampler(x)

        # Calculate acceptance ratio
        log_ratio = target_logpdf(x_prop) - target_logpdf(x)
        if np.log(np.random.random()) < log_ratio:
            x = x_prop
            if i >= burnin:
                accepted += 1

        if i >= burnin:
            samples.append(x)

    acceptance_rate = accepted / n_samples
    print(f"Acceptance rate: {acceptance_rate:.3f}")

    return np.array(samples)

# Example: Sampling from a mixture of Gaussians
def mixture_logpdf(x):
    return np.log(0.3 * norm.pdf(x, -2, 0.5) + 0.7 * norm.pdf(x, 3, 1.5))

def proposal_sampler(x):
    return x + np.random.normal(0, 1)  # Random walk proposal

# Generate samples
samples = metropolis_hastings(mixture_logpdf, proposal_sampler, 0, 5000)

print(f"Sample mean: {np.mean(samples):.3f}")
print(f"Sample std: {np.std(samples):.3f}")</code></pre>

            <div class="my-8">
                <img src="https://via.placeholder.com/800x400/3b82f6/ffffff?text=MCMC+Trace+Plot" alt="MCMC trace plot showing convergence" class="w-full rounded-lg shadow-lg">
                <p class="text-sm text-slate-600 mt-3 text-center italic">Figure 1: Trace plot showing MCMC chain convergence to the target distribution</p>
            </div>

            <h4 class="font-space-grotesk text-2xl font-semibold text-slate-800 mb-4">Hamiltonian Monte Carlo (HMC)</h4>
            <p>
                HMC uses gradient information to propose more efficient moves, especially effective for high-dimensional problems.
                The algorithm simulates a particle moving on a potential energy surface defined by the negative log posterior.
            </p>

            <div class="bg-slate-50 p-6 rounded-lg my-8 border-l-4 border-accent-cyan">
                <p class="text-sm text-slate-600 mb-2 font-medium">Hamiltonian dynamics:</p>
                <div class="space-y-2">
                    <div class="text-center font-mono bg-white p-3 rounded border">\(\frac{d\theta}{dt} = \frac{\partial H}{\partial p} = p\)</div>
                    <div class="text-center font-mono bg-white p-3 rounded border">\(\frac{dp}{dt} = -\frac{\partial H}{\partial \theta} = -\nabla_\theta \log p(\theta | \mathbf{y})\)</div>
                </div>
            </div>

            <h4 class="font-space-grotesk text-2xl font-semibold text-slate-800 mb-4">Practical Considerations</h4>
            <ul class="space-y-3 mb-6">
                <li><strong>Convergence diagnostics:</strong> Always check for convergence using multiple chains and diagnostic tests</li>
                <li><strong>Effective sample size:</strong> High autocorrelation reduces the effective number of independent samples</li>
                <li><strong>Tuning:</strong> Proposal distributions and step sizes significantly impact performance</li>
                <li><strong>Initialization:</strong> Poor starting points can lead to slow mixing or getting stuck in local modes</li>
            </ul>

            <div class="bg-blue-50 border-l-4 border-accent-blue p-6 my-8">
                <h5 class="font-space-grotesk font-semibold text-accent-blue mb-2">üí° Key Takeaway</h5>
                <p class="text-slate-700">
                    MCMC methods transform the problem of computing complex integrals into the problem of simulating
                    correlated random walks. While they require careful implementation and monitoring, they enable
                    Bayesian inference in problems where analytical solutions are impossible.
                </p>
            </div>

            <h4 class="font-space-grotesk text-2xl font-semibold text-slate-800 mb-4">Modern Applications</h4>
            <p>
                In modern probabilistic programming languages like PyMC3, Stan, and Pyro, these algorithms are implemented
                with sophisticated tuning procedures, making Bayesian inference accessible to practitioners across disciplines.
            </p>
            <p>
                The combination of MCMC methods with deep learning architectures has led to Bayesian neural networks,
                enabling uncertainty quantification in complex prediction tasks. As computational power continues to grow,
                we can expect MCMC methods to play an increasingly important role in machine learning and scientific computing.
            </p>
        </article>

        <!-- Article Footer -->
        <footer class="mt-12 pt-8 border-t border-slate-200">
            <div class="flex flex-col sm:flex-row justify-between items-start sm:items-center gap-4">
                <div class="flex items-center gap-4">
                    <a href="index.html" class="text-accent-purple hover:text-purple-700 transition-colors">
                        ‚Üê Back to Blog
                    </a>
                </div>
                <div class="flex items-center gap-4 text-sm text-slate-600">
                    <span>Share:</span>
                    <a href="#" class="hover:text-accent-blue transition-colors">
                        <i class="fab fa-twitter"></i>
                    </a>
                    <a href="#" class="hover:text-accent-blue transition-colors">
                        <i class="fab fa-linkedin"></i>
                    </a>
                    <a href="#" class="hover:text-accent-purple transition-colors">
                        <i class="fas fa-link"></i>
                    </a>
                </div>
            </div>
        </footer>
    </div>

    <!-- Footer -->
    <footer class="mt-16 bg-slate-900 text-white">
        <div class="max-w-4xl mx-auto px-4 py-8">
            <div class="text-center">
                <p class="text-slate-400 mb-4">¬© 2025 Felix Aertebjerg. All rights reserved.</p>
                <div class="flex justify-center space-x-6">
                    <a href="../index.html" class="text-slate-400 hover:text-white transition-colors">Home</a>
                    <a href="index.html" class="text-slate-400 hover:text-white transition-colors">Blog</a>
                    <a href="https://github.com/aerte" class="text-slate-400 hover:text-white transition-colors">GitHub</a>
                    <a href="https://linkedin.com/in/yourusername" class="text-slate-400 hover:text-white transition-colors">LinkedIn</a>
                </div>
            </div>
        </div>
    </footer>

    <!-- Prism.js for syntax highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>